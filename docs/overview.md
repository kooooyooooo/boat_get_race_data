# プロジェクト概要書

## 1. プロジェクト名
競艇データ収集基盤構築プロジェクト

## 2. プロジェクトの目的
- 目的: 機械学習モデル開発および高精度な投票シミュレーション実施に必要な、網羅的かつ構造化された過去10年分の競艇データセットを構築する。
- KPIや数値目標:
    - 過去10年分（2015年1月〜2025年3月）の主要レースデータ（レース結果、詳細、選手・モーター情報、オッズ、展示データなど）の収集率100%。
    - 後続のML開発・シミュレーションフェーズが円滑に進められるデータ品質を確保。

## 3. 対象となる顧客・ユーザー
- MLモデルの学習、評価、および投票戦略の有効性シミュレーションに必要な、クリーンで体系化された過去データがない。
- 既存のデータソースは散在しており、手動でのデータ収集・整形には膨大な時間と労力がかかる。
- 信頼性の高いデータに基づいた分析・検証を行いたい。
- このプロジェクトで提供できる価値:
    - ML開発の基盤となる高品質なデータセットを提供し、データ収集・前処理の負担を大幅に軽減する。
    - 長期間のデータに基づく信頼性の高いシミュレーションを可能にし、投票戦略の有効性を定量的に評価できる環境を提供する。

## 4. プロジェクトの背景・解決すべき課題
- 現状の問題点:
    - 競艇の過去レースデータは公式サイト等に存在するが、API等で簡単に取得できる形式ではなく、ウェブサイト上の情報を個別に収集する必要がある。
    - 選手、モーター、レース場などの関連情報も別々に存在し、これらを結合して分析可能な形にするには複雑な処理が必要。
    - 特に、過去10年分といった長期間にわたるデータを網羅的に収集・整形するのは手動では非現実的。
- ビジネス上の必要性:
    - 統計的に有意な予測モデルを構築し、低リスク戦略であっても利益を出すためには、長期間のデータに基づく robust な分析と検証が不可欠である。
    - データ収集の課題を解決しなければ、後続のML開発・シミュレーションに進むことができない。
- 既存施策や関連プロジェクト:
    - （特になし - プロジェクト推進者による新規取り組み）

## 5. 概要イメージ
- 実装予定の主な機能:
  1. 競艇関連ウェブサイトからのデータ自動収集モジュール（スクレイピング等）
  2. 収集データのクリーニング・正規化・構造化処理モジュール
  3. 収集データのデータベース（RDB等）への格納機能
  4. データ品質チェックおよびモニタリング機能
※ データの流れ：各種データソース → データ収集モジュール → クリーニング・整形モジュール → データベース格納

## 6. 想定される効果
- 定量的効果:
    - 過去10年分の競艇主要データを格納したデータベースの完成。
    - 10年間のシミュレーションに基づく、より信頼性の高い回収率予測値の算出が可能になる。
- 定性的効果:
    - MLモデル開発および投票戦略研究のための強固なデータ基盤の確立。
    - 様々な角度からのデータ分析・洞察獲得の促進。
    - データに基づいた意思決定プロセスの強化。
    - プロジェクト推進者のデータエンジニアリングスキルの向上。
- 目的やKPIとのつながり:
    - 10年分のデータ収集完了（KPI達成）により、目的である「MLモデル開発および高精度な投票シミュレーション」の実施が可能となる。データ品質確保（KPI達成）は、後続フェーズの円滑化とシミュレーション結果の信頼性向上に直結する。

## 7. プロジェクトの進め方（フェーズ構成）
- フェーズ1: 計画・設計
    - 作業内容: 目標とするデータ項目の詳細定義、データソースの特定と調査（ウェブサイト構造、利用規約確認等）、データベーススキーマ設計

- フェーズ2: 開発
    - 作業内容: データ収集スクリプト/モジュールの開発、データクリーニング・整形処理スクリプトの開発、データベース構築、データ取り込みスクリプトの開発。

- フェーズ3: データ収集実行
    - 作業内容: 開発したツールを用いて過去10年分のデータ収集を順次実行、収集状況のモニタリング。

- フェーズ4: データ検証・整備・格納
    - 作業内容: 収集データの品質チェック（欠損、整合性）、エラーデータの修正または除外、最終的なデータ構造への格納、後続フェーズでの利用に向けたデータマート的な整備（必要に応じて）。

## 8. リスク・課題と対応策
- リスク/課題1: データソース側の仕様変更やサービス停止
  - 対応策: 汎用的なスクレイピング技術を使用し、仕様変更に柔軟に対応できる設計とする。定期的なソース監視を行う。代替データソースの可能性を常に考慮する。
- リスク/課題2: 収集データの品質問題（欠損、不整合、誤ったパース）
  - 対応策: 厳密なバリデーションルールをコードに組み込む。収集後の自動/手動での品質チェック体制を設ける。主要項目の欠損データに対する補完/除外ルールを定義する。
- リスク/課題3: ウェブサイトの利用規約違反やサーバーへの過負荷
  - 対応策: 対象サイトの利用規約を遵守する。robots.txtを確認する。過度なアクセスを避け、適切な間隔を空けて収集する。夜間などサーバー負荷の低い時間帯を利用する。
- リスク/課題4: 長期間のデータ収集に伴う技術的な課題（ネットワークエラー、処理の中断、ストレージ容量）
  - 対応策: エラーハンドリングとリトライ機能を実装する。処理の中断を考慮したチェックポイント機能を設ける。必要に応じてクラウドストレージやDBサービスを活用する。

## 9. （必要に応じて追加セクション）
- 技術的要件:
    - 開発言語: Python (推奨)
    - 主要ライブラリ: playwirte, Scrapy, Requests, Pandas (データ処理), , SQLAlchemy (データベース抽象化レイヤーとして強く推奨)
    - データベース: 初期開発・小規模運用時は手軽なSQLiteを採用
        ただし、将来的なPostgreSQL等への移行コストを最小限に抑えるため、データベースとのやり取りはSQLAlchemyなどのORM/抽象化レイヤーを介して行うことを必須とし、特定のDBに依存した直接的なSQL記述は極力避ける。
    - 実行環境: ローカル環境
    - ストレージ: DB容量
- 運用・保守計画: データソースの定期的な監視、収集スクリプトのメンテナンス（仕様変更対応）、DBのバックアップ。