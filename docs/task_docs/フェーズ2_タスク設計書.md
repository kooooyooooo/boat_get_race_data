# フェーズ2：開発（競艇データ収集基盤）

## 1. フェーズの目的・プロジェクト全体の中での位置づけ
- フェーズ1で設計した内容に基づき、競艇データ（出走表、直前情報、結果）を自動収集し、データベースに格納するシステムを開発する。
- プロジェクト全体の「データ収集基盤構築」の中核を担うフェーズであり、後続のデータ収集実行フェーズ（フェーズ3）で利用するツール群を完成させる。

## 2. 範囲・対象
- **対象データ:** 出走表情報、直前情報（気象情報含む）、レース結果情報、払戻金情報
- **対象期間:** プロトタイプ開発時点では指定日付の特定レースのみだが、最終的には過去10年分を扱える設計とする。
- **対象レース場:** 全24場
- **開発対象:**
    - データ収集スクリプト（スクレイピングモジュール）
    - データクリーニング・整形処理スクリプト
    - データベース操作モジュール（SQLAlchemy を使用）
    - データ投入スクリプト
- **範囲外:**
    - ファン手帳データの取り込み（別途検討）
    - 高度なデータ分析機能
    - Web UI

## 3. 主要タスク

- **3.1 スクレイピングモジュールの開発**
  - 目的: 指定されたURLからHTMLを取得し、必要なデータを抽出する機能を提供する。
  - 内容:
    - `requests` または `playwright` を利用したHTML取得処理の実装 (参考: `prototype/scrape_prototype.py` の `fetch_html`)。
    - `BeautifulSoup4` を利用したHTML解析処理の実装。
    - `docs/research/データ項目定義・セレクタマッピング.md` に基づくデータ抽出ロジックの実装 (参考: `prototype/scrape_prototype.py` の `extract_race_info`, `extract_entry_info`, `extract_live_info`)。
    - プロトタイプで未取得だった項目（風向き、部品交換情報など）の抽出ロジック追加。
    - レース結果ページ、払戻金ページのスクレイピングロジック追加。
    - 安定板使用フラグの取得ロジック実装 (参考: `prototype/scrape_prototype.py` の `extract_race_info`)。
    - サイト構造変更に対応しやすいよう、セレクタ情報は設定ファイル等で管理する。
    - `docs/research/クロールポリシー確認結果.md` に基づく負荷対策（リクエスト間隔、リトライ、User-Agent設定）の実装 (参考: `prototype/scrape_prototype.py` の `main` ループ、`fetch_html` のヘッダー設定)。
  - 成果物: スクレイピングを行うPythonモジュール (`src/scraping/`)

- **3.2 データモデルとデータベース操作モジュールの開発**
  - 目的: `docs/design/DB_schema.md` に基づくデータモデルを定義し、データベースへの永続化を行う機能を提供する。
  - 内容:
    - SQLAlchemy を用いた ORM (Object-Relational Mapping) モデルの定義 (`src/models.py`) (参考: `prototype/scrape_prototype.py` の `Venue`, `Race`, `RaceEntry` クラス定義)。
    - `Venue`, `Player`, `Race`, `RaceEntry`, `Payout` テーブルに対応するモデルクラスを作成。
        - `Player` テーブルへの選手基本情報登録ロジックを含む（重複チェック含む）。
    - データベースセッション管理の実装 (参考: `prototype/scrape_prototype.py` の `init_database`, `main` での `sessionmaker` 利用)。
    - データの登録・更新処理を行う関数の実装 (`src/db_handler.py`) (参考: `prototype/scrape_prototype.py` の `main` ループ内のDB保存処理)。
  - 成果物: SQLAlchemy モデル定義ファイル、データベース操作関数群

- **3.3 データクリーニング・整形処理モジュールの開発**
  - 目的: スクレイピングで取得した生データを、データベース格納に適した形式に変換・整形する。
  - 内容:
    - 取得データ型の変換（文字列→数値、日付、時刻など）(参考: `prototype/scrape_prototype.py` の抽出関数内での型変換)。
    - 表記ゆれの統一（例: 全角数字→半角数字）。
    - 単位の除去（例: "kg", "m", "cm", "℃"）(参考: `prototype/scrape_prototype.py` の `extract_live_info` での正規表現利用)。
    - 特殊な値（"F", "L", "失" など）の処理ルール定義と実装。
    - `Race` テーブルの `season_year`, `season_term` の決定ロジック実装 (参考: `prototype/scrape_prototype.py` の `main` 内ロジック)。
    - `RaceEntry` テーブルの `rank` (丸め後着順) の決定ロジック実装。
  - 成果物: データクリーニング・整形を行うPython関数群 (`src/cleaning.py`)

- **3.4 データ投入スクリプトの開発**
  - 目的: スクレイピング、クリーニング、DB保存の処理を連携させ、指定された条件（日付範囲、レース場）のデータ収集を実行するスクリプトを作成する。
  - 内容:
    - コマンドライン引数等で実行条件（日付、レース場、レース番号）を指定できるインターフェース。
    - 開催日程ページのスクレイピングによる対象レース特定ロジック (参考: `prototype/scrape_prototype.py` の `get_active_venues`, `main` での絞り込み)。
    - 対象レースごとに各モジュール（スクレイピング、クリーニング、DB保存）を呼び出す処理フローの実装 (参考: `prototype/scrape_prototype.py` の `main` 関数)。
    - エラーハンドリング（特定レースの失敗時に処理を継続するなど）。
    - 処理状況のロギング機能 (参考: `prototype/scrape_prototype.py` での `logging` 利用)。
  - 成果物: データ収集実行スクリプト (`src/main_scraper.py`)

- **3.5 データベース構築**
  - 目的: 開発およびデータ収集実行に使用するSQLiteデータベースファイルを初期構築する。
  - 内容:
    - SQLAlchemy を使用して `docs/design/DB_schema.md` に定義されたテーブルスキーマを作成するスクリプト (`src/init_db.py`) (参考: `prototype/scrape_prototype.py` の `init_database`)。
    - `venue` テーブルへの初期データ投入 (参考: `prototype/scrape_prototype.py` の `init_database`)。
  - 成果物: データベース初期化スクリプト、空のデータベースファイル (`data/boat_data.db`)

## 4. 成果物
1. スクレイピングモジュール (`src/scraping/`)
2. データモデル定義とデータベース操作モジュール (`src/models.py`, `src/db_handler.py`)
3. データクリーニング・整形処理モジュール (`src/cleaning.py`)
4. データ投入実行スクリプト (`src/main_scraper.py`)
5. データベース初期化スクリプト (`src/init_db.py`)
6. ユニットテストコード (可能な範囲で)
7. 更新されたREADME (`README.md`) - セットアップ方法、実行方法を記載

## 5. 受け入れ基準
- 指定した日付・レース場の出走表、直前情報、レース結果、払戻金データが、定義されたDBスキーマに従って正しくデータベースに格納されること。
- プロトタイプで判明した課題（風向き、部品交換情報取得）が解決されていること。
- クロールポリシーを遵守し、サーバーに過度な負荷をかけない実装になっていること（リクエスト間隔設定など）。
- コードに基本的なエラーハンドリングが実装されており、特定のエラーで全体の処理が停止しないこと。
- README に従って環境構築とスクリプト実行が可能であること。

## 6. タスク一覧
| No. | タスク                         | 実施内容詳細                                  | 参照ドキュメント                                                                                                                                                              |
| ---- | ---------------------------- | --------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 6.1  | スクレイピングモジュール開発   | 出走表・直前・結果・払戻ページのデータ抽出、負荷対策実装 (参考: `prototype`の`fetch_html`, `extract_*`関数群, `main`ループ) | `docs/research/データ項目定義・セレクタマッピング.md`, `docs/research/クロールポリシー確認結果.md`, `docs/task_docs/プロトタイプ報告書.md`, `src/prototype/scrape_prototype.py` |
| 6.2  | DBモデル・操作モジュール開発  | SQLAlchemyモデル定義、CRUD操作実装、Player登録実装 (参考: `prototype`のモデル定義, `init_database`, `main`のDB操作部) | `docs/design/DB_schema.md`, `src/prototype/scrape_prototype.py`                                                                                                                     |
| 6.3  | クリーニング・整形モジュール開発 | データ型変換、表記ゆれ統一、特殊値処理、`season_year/term`, `rank`算出 (参考: `prototype`の抽出関数内の処理, `main`のseason算出部) | `docs/research/データ項目定義・セレクタマッピング.md`, `docs/design/DB_schema.md`, `src/prototype/scrape_prototype.py`                                                         |
| 6.4  | データ投入スクリプト開発     | 実行引数処理、処理フロー制御、ロギング、エラーハンドリング (参考: `prototype`の`main`, `get_active_venues`, ロギング設定) | (各モジュール仕様), `src/prototype/scrape_prototype.py`                                                                                                                                    |
| 6.5  | データベース構築             | テーブル作成、Venue初期データ投入 (参考: `prototype`の`init_database`) | `docs/design/DB_schema.md`, `src/prototype/scrape_prototype.py`                                                                                                                     |
| 6.6  | 結合テスト・デバッグ         | 一連の処理フローの動作確認、不具合修正            | (全ドキュメント)                                                                                                                                                              |
| 6.7  | ドキュメント整備             | README更新、コードコメント                     | `README.md`                                                                                                                                                                   |

## 7. リスクと課題
- **サイト構造変更リスク**: スクレイピング対象サイトのHTML構造が変更されると、データ取得が失敗する。
    - 対応策: セレクタを外部設定ファイル化する。定期的な動作確認とメンテナンス体制。
- **IP BANリスク**: 短時間に大量アクセスを行うと、アクセス元IPアドレスがブロックされる可能性がある。
    - 対応策: 適切なリクエスト間隔の設定、リトライ処理の実装、User-Agentの設定。時間帯を考慮した実行。
- **データ欠損・不整合**: スクレイピング漏れや予期せぬデータ形式により、DBに欠損や不整合が生じる可能性がある。
    - 対応策: 取得データに対するバリデーション強化。エラーログの詳細化。収集後のデータ品質チェック（フェーズ4）。
- **開発遅延リスク**: 特定箇所のスクレイピング難易度が高い、予期せぬ技術課題が発生する。
    - 対応策: 不明点は早期に相談。代替ライブラリの検討 (例: `requests` で取得できない場合の `playwright` への切り替え)。

## 8. 前提条件・依存
- フェーズ1の成果物（DBスキーマ、セレクタマッピング等）がFIXしていること。
- 開発環境（Python, venv, 必要なライブラリ）が準備されていること (`docs/overview.md` 参照)。
- スクレイピング対象のWebサイトが利用可能であること。

## 9. 関連資料・参照
- `docs/overview.md` (プロジェクト概要)
- `docs/design/DB_schema.md` (DBスキーマ)
- `docs/research/データ項目定義・セレクタマッピング.md` (セレクタ情報)
- `docs/task_docs/プロトタイプ報告書.md` (プロトタイプの課題)
- `docs/research/クロールポリシー確認結果.md` (スクレイピングポリシー)
- `docs/task_docs/フェーズ1_タスク設計書.md` (フェーズ1の成果)
- `src/prototype/scrape_prototype.py` (プロトタイプ実装) 